diff --git a/build.sbt b/build.sbt
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/build.sbt
@@ -0,0 +1,14 @@
+ThisBuild / scalaVersion := "2.12.18"
+lazy val versions = new { val beam="2.58.0"; val gcloudBq="2.43.2"; val slf4j="2.0.12"; val gson="2.10.1"; val snake="2.0"; val awsSdk="2.25.62"; val secret="2.30.0" }
+lazy val root = (project in file(".")).aggregate(framework, memberPipeline).settings(name := "gpt5-helper-cdc-realtime", version := "0.4.0")
+lazy val commonLibs = Seq(
+  "org.apache.beam" % "beam-sdks-java-core" % versions.beam,
+  "org.apache.beam" % "beam-runners-google-cloud-dataflow-java" % versions.beam,
+  "com.google.cloud" % "google-cloud-bigquery" % versions.gcloudBq,
+  "com.google.cloud" % "google-cloud-secretmanager" % versions.secret,
+  "software.amazon.awssdk" % "s3" % versions.awsSdk,
+  "com.google.code.gson" % "gson" % versions.gson,
+  "org.yaml" % "snakeyaml" % versions.snake,
+  "org.slf4j" % "slf4j-api" % versions.slf4j)
+lazy val framework = (project in file("framework")).settings(name := "cdc-framework", libraryDependencies ++= commonLibs)
+lazy val memberPipeline = (project in file("business-domains/member")).dependsOn(framework).settings(name := "member-pipeline")

diff --git a/framework/src/main/scala/com/analytics/framework/utils/ConfigLoader.scala b/framework/src/main/scala/com/analytics/framework/utils/ConfigLoader.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/utils/ConfigLoader.scala
@@ -0,0 +1,13 @@
+package com.analytics.framework.utils
+import org.yaml.snakeyaml.Yaml
+import java.io.FileInputStream
+import scala.jdk.CollectionConverters._
+import java.util.{Map => JMap}
+object ConfigLoader {
+  def load(path: String): JMap[String, Object] = {
+    val yaml = new Yaml()
+    val in = new FileInputStream(path)
+    try yaml.load(in).asInstanceOf[JMap[String, Object]]
+    finally in.close()
+  }
+}

diff --git a/framework/src/main/scala/com/analytics/framework/connectors/S3SnapshotReader.scala b/framework/src/main/scala/com/analytics/framework/connectors/S3SnapshotReader.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/connectors/S3SnapshotReader.scala
@@ -0,0 +1,18 @@
+package com.analytics.framework.connectors
+import software.amazon.awssdk.services.s3.S3Client
+import software.amazon.awssdk.services.s3.model._
+import scala.jdk.CollectionConverters._
+import java.nio.charset.StandardCharsets
+class S3SnapshotReader(bucket: String, prefix: String) {
+  @transient private var s3: S3Client = _
+  def init(): Unit = { s3 = S3Client.create() }
+  def listKeys(limit: Int = 64): List[String] = {
+    val req = ListObjectsV2Request.builder().bucket(bucket).prefix(prefix).maxKeys(limit).build()
+    s3.listObjectsV2(req).contents().asScala.map(_.key()).toList
+  }
+  def readObjectLines(key: String): Iterator[String] = {
+    val get = GetObjectRequest.builder().bucket(bucket).key(key).build()
+    val in = s3.getObject(get)
+    scala.io.Source.fromInputStream(in, StandardCharsets.UTF_8.name()).getLines()
+  }
+}

diff --git a/framework/src/main/scala/com/analytics/framework/connectors/SecretManager.scala b/framework/src/main/scala/com/analytics/framework/connectors/SecretManager.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/connectors/SecretManager.scala
@@ -0,0 +1,13 @@
+package com.analytics.framework.connectors
+import com.google.cloud.secretmanager.v1.{AccessSecretVersionRequest, SecretManagerServiceClient}
+object SecretManager {
+  def access(projectId: String, secretId: String, version: String = "latest"): String = {
+    val name = s"projects/$projectId/secrets/$secretId/versions/$version"
+    val client = SecretManagerServiceClient.create()
+    try {
+      val req = AccessSecretVersionRequest.newBuilder().setName(name).build()
+      val res = client.accessSecretVersion(req)
+      new String(res.getPayload.getData.toByteArray, java.nio.charset.StandardCharsets.UTF_8)
+    } finally client.close()
+  }
+}

diff --git a/framework/src/main/scala/com/analytics/framework/modules/dq/DataQuality.scala b/framework/src/main/scala/com/analytics/framework/modules/dq/DataQuality.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/modules/dq/DataQuality.scala
@@ -0,0 +1,28 @@
+package com.analytics.framework.modules.dq
+import org.apache.beam.sdk.transforms.DoFn
+import com.analytics.framework.pipeline.model.CDCRecord
+import scala.jdk.CollectionConverters._
+sealed trait Rule
+case class NotNull(columns: List[String]) extends Rule
+case class RegexRule(column: String, pattern: String) extends Rule
+case class RangeRule(column: String, min: Double, max: Double) extends Rule
+case class DQResult(passed: Boolean, rule: String, detail: String)
+class DataQualityDoFn(rules: List[Rule]) extends DoFn[CDCRecord, (CDCRecord, DQResult)] {
+  @DoFn.ProcessElement
+  def process(ctx: DoFn[CDCRecord, (CDCRecord, DQResult)]#ProcessContext): Unit = {
+    val rec = ctx.element(); val data = rec.data.asScala.toMap
+    var ok = true; val details = scala.collection.mutable.ListBuffer[String]()
+    rules.foreach {
+      case NotNull(cols) =>
+        val bad = cols.filter(c => !data.contains(c) || data(c) == null)
+        if (bad.nonEmpty) { ok = false; details += s"not_null_missing=${bad.mkString(",")}" }
+      case RegexRule(col, pat) =>
+        val v = data.get(col).map(_.toString).getOrElse("")
+        if (!v.matches(pat)) { ok = false; details += s"regex_failed=$col" }
+      case RangeRule(col, min, max) =>
+        val d = data.get(col).map(_.toString.toDoubleOption.getOrElse(Double.NaN)).getOrElse(Double.NaN)
+        if (d.isNaN || d < min || d > max) { ok = false; details += s"range_failed=$col" }
+    }
+    ctx.output((rec, DQResult(ok, "combined", details.mkString(";"))))
+  }
+}

diff --git a/framework/src/main/scala/com/analytics/framework/modules/reconcile/Reconciliation.scala b/framework/src/main/scala/com/analytics/framework/modules/reconcile/Reconciliation.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/modules/reconcile/Reconciliation.scala
@@ -0,0 +1,33 @@
+package com.analytics.framework.modules.reconcile
+import org.apache.beam.sdk.transforms.DoFn
+import org.apache.beam.sdk.transforms.windowing.BoundedWindow
+import scala.jdk.CollectionConverters._
+import com.analytics.framework.connectors.S3SnapshotReader
+import java.time.Instant
+case class ReconcileInput(id: String, table: String, zone: String, gcpRow: java.util.Map[String, AnyRef])
+case class ReconcileLog(recordId: String, table: String, zone: String, gcpCount: Long, awsCount: Long, countMatch: Boolean, diffs: List[Map[String, String]], ts: Long)
+class ReconcileDoFn(bucket: String, prefix: String, idColumn: String, sampleLimit: Int = 5) extends DoFn[ReconcileInput, ReconcileLog] {
+  @transient private var s3: S3SnapshotReader = _
+  @DoFn.Setup def setup(): Unit = { s3 = new S3SnapshotReader(bucket, prefix); s3.init() }
+  @DoFn.ProcessElement
+  def process(ctx: DoFn[ReconcileInput, ReconcileLog]#ProcessContext, w: BoundedWindow): Unit = {
+    val e = ctx.element()
+    val keys = s3.listKeys(32); val rid = e.id
+    val awsRows: List[Map[String, Any]] =
+      keys.iterator.flatMap { k =>
+        s3.readObjectLines(k).take(2000).flatMap { line =>
+          import io.circe.parser._
+          parse(line).toOption.flatMap(_.asObject).map(_.toMap.map { case (k,v) => k -> (if(v.isNull) null else v.toString)})
+            .filter(_.getOrElse(idColumn, "") == rid)
+        }
+      }.take(sampleLimit).toList
+    val gcpMap = e.gcpRow.asScala.toMap
+    val gcpCount = 1L; val awsCount = awsRows.size.toLong; val countMatch = gcpCount == awsCount
+    val diffs = awsRows.flatMap { r =>
+      val cols = (r.keySet ++ gcpMap.keySet).filterNot(_.startsWith("_"))
+      val bad = cols.filter(c => Option(gcpMap.getOrElse(c, null)).map(_.toString).orNull != Option(r.getOrElse(c, null)).map(_.toString).orNull)
+      if (bad.nonEmpty) Some(Map("columns" -> bad.mkString(","))) else None
+    }
+    ctx.output(ReconcileLog(rid, e.table, e.zone, gcpCount, awsCount, countMatch, diffs, Instant.now().toEpochMilli))
+  }
+}

diff --git a/framework/src/main/scala/com/analytics/framework/pipeline/model/Models.scala b/framework/src/main/scala/com/analytics/framework/pipeline/model/Models.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/pipeline/model/Models.scala
@@ -0,0 +1,3 @@
+package com.analytics.framework.pipeline.model
+case class Notification(messageId: String, eventType: String, accountId: String, ts: String)
+case class CDCRecord(recordId: String, op: String, data: java.util.Map[String, AnyRef], tableName: String, zone: String)

diff --git a/framework/src/main/scala/com/analytics/framework/pipeline/io/NotificationParser.scala b/framework/src/main/scala/com/analytics/framework/pipeline/io/NotificationParser.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/pipeline/io/NotificationParser.scala
@@ -0,0 +1,16 @@
+package com.analytics.framework.pipeline.io
+import org.apache.beam.sdk.io.gcp.pubsub.PubsubMessage
+import org.apache.beam.sdk.transforms.DoFn
+import scala.jdk.CollectionConverters._
+import com.analytics.framework.pipeline.model.Notification
+class NotificationParser extends DoFn[PubsubMessage, Notification] {
+  @DoFn.ProcessElement
+  def process(ctx: DoFn[PubsubMessage, Notification]#ProcessContext): Unit = {
+    val m = ctx.element()
+    val attr = m.getAttributeMap.asScala
+    val accountId = attr.getOrElse("accountId", "")
+    val eventType = attr.getOrElse("event_type", if (attr.getOrElse("topic","") contains "create") "create" else "update")
+    val ts = attr.getOrElse("event_timestamp", java.time.Instant.now().toString)
+    ctx.output(Notification(attr.getOrElse("message_id", java.util.UUID.randomUUID().toString), eventType, accountId, ts))
+  }
+}

diff --git a/framework/src/main/scala/com/analytics/framework/pipeline/process/MemberProfileApiFetcher.scala b/framework/src/main/scala/com/analytics/framework/pipeline/process/MemberProfileApiFetcher.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/pipeline/process/MemberProfileApiFetcher.scala
@@ -0,0 +1,32 @@
+package com.analytics.framework.pipeline.process
+import org.apache.beam.sdk.transforms.DoFn
+import com.analytics.framework.pipeline.model.{Notification, CDCRecord}
+import com.analytics.framework.connectors.SecretManager
+import java.net.http.{HttpClient, HttpRequest, HttpResponse}
+import java.net.URI
+import java.time.Duration
+import com.google.gson.{Gson, JsonParser}
+import scala.jdk.CollectionConverters._
+class MemberProfileApiFetcher(projectId: String, baseUrl: String, secretId: String, timeoutSec: Int = 10, rateLimitRps: Int = 30)
+  extends DoFn[Notification, CDCRecord] {
+  @transient private var http: HttpClient = _
+  @DoFn.Setup def setup(): Unit = { http = HttpClient.newBuilder().connectTimeout(Duration.ofSeconds(timeoutSec.toLong)).build() }
+  @DoFn.ProcessElement
+  def process(ctx: DoFn[Notification, CDCRecord]#ProcessContext): Unit = {
+    val n = ctx.element(); if (n.accountId == null || n.accountId.isEmpty) return
+    val token = SecretManager.access(projectId, secretId, "latest").trim
+    val url = s"$baseUrl/accounts/${n.accountId}?profileRole=OWNER&pageNumber=1&pageSize=10"
+    val req = HttpRequest.newBuilder().uri(URI.create(url)).timeout(Duration.ofSeconds(timeoutSec)).header("Authorization", s"Bearer $token").header("Accept", "application/json").GET().build()
+    val res = http.send(req, HttpResponse.BodyHandlers.ofString())
+    if (res.statusCode() >= 200 && res.statusCode() < 300) {
+      val body = res.body()
+      val parser = new JsonParser(); val je = parser.parse(body)
+      val map = new java.util.HashMap[String, AnyRef]()
+      val obj = if (je.isJsonArray) je.getAsJsonArray.get(0).getAsJsonObject else je.getAsJsonObject
+      for (e <- obj.entrySet().asScala) map.put(e.getKey, if (e.getValue.isJsonNull) null else e.getValue.getAsString)
+      ctx.output(CDCRecord(n.accountId, "upsert", map, "s_loy_program", "raw"))
+    } else {
+      // TODO: DLQ
+    }
+  }
+}

diff --git a/framework/src/main/scala/com/analytics/framework/pipeline/core/Orchestrator.scala b/framework/src/main/scala/com/analytics/framework/pipeline/core/Orchestrator.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/pipeline/core/Orchestrator.scala
@@ -0,0 +1,52 @@
+package com.analytics.framework.pipeline.core
+import org.apache.beam.sdk.Pipeline
+import org.apache.beam.sdk.options.PipelineOptionsFactory
+import org.apache.beam.sdk.transforms.{ParDo, Flatten, DoFn}
+import org.apache.beam.sdk.io.gcp.pubsub.{PubsubIO, PubsubMessage}
+import org.apache.beam.sdk.transforms.windowing._
+import org.joda.time.Duration
+import com.analytics.framework.pipeline.io.NotificationParser
+import com.analytics.framework.pipeline.process.MemberProfileApiFetcher
+import com.analytics.framework.pipeline.model.{CDCRecord, Notification}
+import com.analytics.framework.utils.ConfigLoader
+import com.analytics.framework.modules.reconcile.{ReconcileDoFn, ReconcileLog}
+import com.analytics.framework.modules.dq.{DataQualityDoFn, NotNull}
+import org.apache.beam.sdk.io.TextIO
+class Orchestrator(projectId: String, configPath: String) {
+  def build(): Pipeline = {
+    val cfg = ConfigLoader.load(configPath)
+    val p = Pipeline.create(PipelineOptionsFactory.create())
+    val domain = cfg.getOrDefault("domain", "member").toString
+    val logs = cfg.get("logs").asInstanceOf[java.util.Map[String,Object]]
+    val auditBucket = logs.get("audit_bucket").toString
+    val pubsub = cfg.get("pubsub").asInstanceOf[java.util.Map[String,Object]]
+    val subCreate = pubsub.getOrDefault("subscription_create", "member-events-create-sub").toString
+    val subUpdate = pubsub.getOrDefault("subscription_update", "member-events-update-sub").toString
+    val api = cfg.get("api").asInstanceOf[java.util.Map[String,Object]]
+    val baseUrl = api.get("base_url").toString; val secretId = api.get("token_secret_id").toString
+    val timeout = api.getOrDefault("timeout_sec", Int.box(10)).asInstanceOf[Int]; val rps = api.getOrDefault("rate_limit_rps", Int.box(30)).asInstanceOf[Int]
+    val create = p.apply("ReadCreate", PubsubIO.readMessagesWithAttributes().fromSubscription(s"projects/$projectId/subscriptions/$subCreate"))
+    val update = p.apply("ReadUpdate", PubsubIO.readMessagesWithAttributes().fromSubscription(s"projects/$projectId/subscriptions/$subUpdate"))
+    def parse(stream: org.apache.beam.sdk.values.PCollection[PubsubMessage]) =
+      stream.apply("Parse", ParDo.of(new NotificationParser()))
+        .apply("Window1m", org.apache.beam.sdk.transforms.windowing.Window.into[Notification](FixedWindows.of(Duration.standardMinutes(1)))
+          .triggering(AfterWatermark.pastEndOfWindow().withEarlyFirings(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.standardSeconds(30))))
+          .withAllowedLateness(Duration.standardMinutes(10)).discardingFiredPanes())
+    val parsed = org.apache.beam.sdk.values.PCollectionList.of(parse(create)).and(parse(update)).apply("Merge", Flatten.pCollections())
+    val fetched = parsed.apply("FetchMemberAPI", ParDo.of(new MemberProfileApiFetcher(projectId, baseUrl, secretId, timeout, rps)))
+    val recon = cfg.get("reconciler").asInstanceOf[java.util.Map[String,Object]]
+    val awsBucket = recon.get("aws_bucket").toString; val rawPrefix = recon.get("prefix_raw").toString
+    val reconLogs = fetched.apply("ReconcileRaw", ParDo.of(new ReconcileDoFn(awsBucket, rawPrefix, "accountId")))
+    val reconPath = s"$auditBucket/reconcile_log/$domain/raw"
+    reconLogs.apply("ReconToJson", ParDo.of(new DoFn[ReconcileLog, String](){ @DoFn.ProcessElement def p(ctx: DoFn[ReconcileLog, String]#ProcessContext): Unit = { val r = ctx.element(); val json = "{{\"recordId\":\""+r.recordId+"\",\"table\":\""+r.table+"\",\"zone\":\""+r.zone+"\",\"gcpCount\":"+r.gcpCount+",\"awsCount\":"+r.awsCount+",\"countMatch\":"+r.countMatch+",\"ts\":"+r.ts+"}}"; ctx.output(json) }}))
+      .apply("WriteRecon", TextIO.write().to(reconPath).withWindowedWrites().withNumShards(1).withSuffix(".jsonl"))
+    val dq = fetched.apply("DQ", ParDo.of(new DataQualityDoFn(List(NotNull(List("accountId"))))))
+    val dqPath = s"$auditBucket/data_quality_log/$domain/raw"
+    dq.apply("DQToJson", ParDo.of(new DoFn[(CDCRecord, com.analytics.framework.modules.dq.DQResult), String](){ @DoFn.ProcessElement def p(ctx: DoFn[(CDCRecord, com.analytics.framework.modules.dq.DQResult), String]#ProcessContext): Unit = { val t = ctx.element(); val passed = t._2.passed; val json = "{{\"recordId\":\""+t._1.recordId+"\",\"passed\":"+passed+",\"detail\":\""+t._2.detail.replace("\\","\\\\").replace("\"","\\\"")+"\",\"zone\":\"raw\"}}"; ctx.output(json) }}))
+      .apply("WriteDQ", TextIO.write().to(dqPath).withWindowedWrites().withNumShards(1).withSuffix(".jsonl"))
+    val pipePath = s"$auditBucket/pipeline_log/$domain/raw"
+    parsed.apply("PipelineLog", ParDo.of(new DoFn[Notification, String](){ @DoFn.ProcessElement def p(ctx: DoFn[Notification, String]#ProcessContext): Unit = { val n = ctx.element(); ctx.output("{{\"messageId\":\""+n.messageId+"\",\"eventType\":\""+n.eventType+"\",\"ts\":\""+n.ts+"\"}}") }}))
+      .apply("WritePipe", TextIO.write().to(pipePath).withWindowedWrites().withNumShards(1).withSuffix(".jsonl"))
+    p
+  }
+}

diff --git a/business-domains/member/src/main/scala/com/analytics/member/MemberPipeline.scala b/business-domains/member/src/main/scala/com/analytics/member/MemberPipeline.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/business-domains/member/src/main/scala/com/analytics/member/MemberPipeline.scala
@@ -0,0 +1,9 @@
+package com.analytics.member
+import com.analytics.framework.pipeline.core.Orchestrator
+object MemberPipeline {
+  def main(args: Array[String]): Unit = {
+    val projectId = sys.env.getOrElse("GCP_PROJECT", "demo-the-1")
+    val configPath = sys.props.getOrElse("config", "business-domains/member/resources/pipeline_config.yaml")
+    val p = new Orchestrator(projectId, configPath).build(); p.run()
+  }
+}

diff --git a/business-domains/member/resources/pipeline_config.yaml b/business-domains/member/resources/pipeline_config.yaml
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/business-domains/member/resources/pipeline_config.yaml
@@ -0,0 +1,29 @@
+project_id: demo-the-1
+region: asia-southeast1
+domain: member
+pubsub:
+  topic_create: member-events-create
+  topic_update: member-events-update
+  subscription_create: member-events-create-sub
+  subscription_update: member-events-update-sub
+  dlq:
+    mode: pubsub
+    subscription_create: member-events-create-dlq-sub
+    subscription_update: member-events-update-dlq-sub
+    gcs_bucket: demo-the-1-dlq
+datasets:
+  raw: member_raw
+  structure: member_structure
+  refined: member_refined
+  analytics: member_analytics
+logs:
+  audit_bucket: gs://demo-the-1-audit
+  window_id_pattern: yyyyMMddHHmm
+api:
+  base_url: https://api.example.com/v1
+  token_secret_id: member_api_token
+  timeout_sec: 10
+  rate_limit_rps: 30
+reconciler:
+  aws_bucket: eagleeye
+  prefix_raw: eagleeye/member/raw/dt=YYYY-MM-DD/

diff --git a/docs/CONFIG.md b/docs/CONFIG.md
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/docs/CONFIG.md
@@ -0,0 +1,4 @@
+# CONFIG
+- `pubsub.topic_*` and `subscription_*` allow switching between create/update streams.
+- `logs.audit_bucket` is a **gs://** URI and must exist before running pipelines.
+- `api.token_secret_id` must exist in Secret Manager with a valid Bearer token.

