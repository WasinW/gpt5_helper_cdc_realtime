--- a/framework/src/main/scala/com/analytics/framework/connectors/s3/S3JsonlReader.scala
+++ b/framework/src/main/scala/com/analytics/framework/connectors/s3/S3JsonlReader.scala
@@ -1,17 +1,134 @@
 package com.analytics.framework.connectors.s3
-import com.fasterxml.jackson.databind.ObjectMapper
+
+import com.fasterxml.jackson.databind.{JsonNode, ObjectMapper}
+import com.fasterxml.jackson.databind.node.ObjectNode
+import com.fasterxml.jackson.core.JsonFactory
+import com.fasterxml.jackson.module.scala.DefaultScalaModule
+
+import java.io.{BufferedReader, InputStreamReader}
+import java.nio.charset.StandardCharsets
 import scala.collection.JavaConverters._
-class S3JsonlReader {
-  private val om = new ObjectMapper()
-  def toMap(line:String): Map[String,Any] = {
-    val m = om.readValue(line, classOf[java.util.Map[String,Object]])
-    m.asScala.toMap
+import scala.util.Try
+
+// AWS SDK v2
+import software.amazon.awssdk.auth.credentials.{DefaultCredentialsProvider, AwsCredentialsProvider}
+import software.amazon.awssdk.regions.Region
+import software.amazon.awssdk.services.s3.S3Client
+import software.amazon.awssdk.services.s3.model.{GetObjectRequest, ListObjectsV2Request}
+import software.amazon.awssdk.core.sync.RequestBody
+import software.amazon.awssdk.core.ResponseInputStream
+
+object S3JsonlReader {
+  private val om = new ObjectMapper(new JsonFactory()).registerModule(DefaultScalaModule)
+
+  private def toDate(windowId:String): String = {
+    // Accept yyyyMMddHHmm, yyyyMMddHH, yyyyMMdd; slice safely
+    val digits = windowId.takeWhile(_.isDigit)
+    if (digits.length >= 12) digits.substring(0,12)
+    else if (digits.length >= 10) digits.substring(0,10)
+    else digits
   }
-  private def toDate(windowId:String): String = {
-    val Re = raw"(\\d{4})(\\d{2})(\\d{2}).*".r
-    windowId match {
-      case Re(y,m,d) => s"$y-$m-$d"
-      case _ => windowId
+
+  private case class S3Url(bucket:String, keyPrefix:String)
+  private def parseS3Uri(uri:String): S3Url = {
+    // s3://bucket/prefix/...
+    val noScheme = uri.stripPrefix("s3://")
+    val p = noScheme.indexOf('/')
+    if (p < 0) S3Url(noScheme, "")
+    else S3Url(noScheme.substring(0,p), noScheme.substring(p+1))
+  }
+
+  def readJsonlForTable(projectId:String, cfg: java.util.Map[String,AnyRef], zone:String, table:String, windowId:String): List[Map[String,Any]] = {
+    val conf = cfg.asScala.toMap
+    // Allow override via mock local directory for tests
+    conf.get("mock_s3_dir") match {
+      case Some(dirAny) =>
+        val dir = String.valueOf(dirAny)
+        val sub = java.nio.file.Paths.get(dir, zone, table, toDate(windowId))
+        if (!java.nio.file.Files.exists(sub)) return Nil
+        val files = java.nio.file.Files.list(sub).iterator().asScala.toList
+        files.flatMap { p =>
+          val br = java.nio.file.Files.newBufferedReader(p, StandardCharsets.UTF_8)
+          try readJsonLines(br) finally br.close()
+        }
+      case None =>
+        (for {
+          urlAny <- conf.get("aws_snapshot_s3").toList
+        } yield {
+          val base = String.valueOf(urlAny)
+          val basePath = if (base.endsWith("/")) base.dropRight(1) else base
+          val prefixUri = s"""$basePath/$zone/$table/${toDate(windowId)}/"""
+          readFromS3(prefixUri)
+        }).flatten
+    }
+  }
+
+  private def readJsonLines(br: BufferedReader): List[Map[String,Any]] = {
+    val out = scala.collection.mutable.ListBuffer.empty[Map[String,Any]]
+    var line: String = null
+    while ({ line = br.readLine(); line != null }) {
+      val t = line.trim
+      if (t.nonEmpty) {
+        Try(om.readTree(t)).toOption.foreach { node =>
+          out += nodeToScala(node)
+        }
+      }
+    }
+    out.toList
+  }
+
+  private def nodeToScala(n: JsonNode): Map[String,Any] = {
+    import scala.collection.mutable
+    val m = mutable.Map.empty[String,Any]
+    val it = n.fields()
+    while (it.hasNext) {
+      val e = it.next()
+      val key = e.getKey
+      val v = e.getValue
+      val scalaV: Any =
+        if (v.isNull) null
+        else if (v.isTextual) v.asText()
+        else if (v.isNumber) v.numberValue()
+        else if (v.isBoolean) java.lang.Boolean.valueOf(v.asBoolean())
+        else if (v.isArray) v.elements().asScala.map(nodeToScalaAny).toList.asJava // keep lists as java for BQ later
+        else if (v.isObject) nodeToScala(v)
+        else v.asText()
+      m += key -> scalaV
+    }
+    m.toMap
+  }
+
+  private def nodeToScalaAny(n: JsonNode): Any =
+    if (n.isNull) null
+    else if (n.isTextual) n.asText()
+    else if (n.isNumber) n.numberValue()
+    else if (n.isBoolean) java.lang.Boolean.valueOf(n.asBoolean())
+    else if (n.isObject) nodeToScala(n)
+    else if (n.isArray) n.elements().asScala.map(nodeToScalaAny).toList.asJava
+    else n.asText()
+
+  private def readFromS3(prefixUri:String): List[Map[String,Any]] = {
+    val s3Url = parseS3Uri(prefixUri)
+    val region = sys.env.get("AWS_REGION").map(Region.of).getOrElse(Region.AP_SOUTHEAST_1)
+    val s3 = S3Client.builder()
+      .credentialsProvider(DefaultCredentialsProvider.create())
+      .region(region)
+      .build()
+    try {
+      val req = ListObjectsV2Request.builder()
+        .bucket(s3Url.bucket)
+        .prefix(s3Url.keyPrefix)
+        .build()
+      val listing = s3.listObjectsV2(req)
+      val keys = listing.contents().asScala.map(_.key()).filter(_.endsWith(".jsonl")).toList
+      keys.flatMap { key =>
+        val gor = GetObjectRequest.builder().bucket(s3Url.bucket).key(key).build()
+        val in: ResponseInputStream[_] = s3.getObject(gor)
+        val br = new BufferedReader(new InputStreamReader(in, StandardCharsets.UTF_8))
+        try readJsonLines(br) finally br.close()
+      }
+    } finally {
+      Try(s3.close())
     }
   }
 }
--- a/framework/src/main/scala/com/analytics/framework/pipeline/stages/ReconcileStage.scala
+++ b/framework/src/main/scala/com/analytics/framework/pipeline/stages/ReconcileStage.scala
@@ -1,19 +1,92 @@
 package com.analytics.framework.pipeline.stages
-import com.analytics.framework.core.base.{BaseStage, PipelineCtx}
+
 import org.apache.beam.sdk.Pipeline
 import org.apache.beam.sdk.values.PCollection
-import org.apache.beam.sdk.transforms.{DoFn, ParDo}
-import org.apache.beam.sdk.transforms.DoFn.ProcessElement
-final case class ReconCfg(zone:String, table:String, mapping: Map[String,String])
-class ReconcileStage[T <: Map[String,Any]](cfg: ReconCfg, sample: Iterable[Map[String,Any]], audit: String => Unit)
-  extends BaseStage[T,T]{
-  override def name: String = s"reconcile:${cfg.zone}.${cfg.table}"
-  def apply(p:Pipeline, in:PCollection[T])(implicit ctx:PipelineCtx): PCollection[T] = {
-    in.apply(name, ParDo.of(new DoFn[T,T](){
-      @ProcessElement def proc(c: DoFn[T,T]#ProcessContext): Unit = {
-        audit(s"""{"zone":"${cfg.zone}","table":"${cfg.table}","status":"checked"}""")
-        c.output(c.element())
+import org.apache.beam.sdk.transforms.{DoFn, ParDo, View, Count}
+import com.analytics.framework.core.base.PipelineCtx
+import scala.collection.JavaConverters._
+
+/**
+ * @param zone       "structure" | "refined" | "analytics"
+ * @param table      table name
+ * @param g2sMapping mapping of GCP-field -> S3-field (for value comparisons). Must include an id mapping (e.g. "id"->"id")
+ * @param sample     up to N sample records from S3 snapshot (already as Scala maps)
+ * @param audit      side-effect logger (e.g., write to GCS/GCS logs)
+ */
+class ReconcileStage[T <: Map[String,Any]]
+  (zone:String,
+   table:String,
+   g2sMapping: Map[String,String],
+   sample: List[Map[String,Any]],
+   audit: String => Unit
+  ) {
+
+  private val idKey: Option[(String,String)] =
+    g2sMapping.find { case (g,s) => g.equalsIgnoreCase("id") || g.toLowerCase.endsWith("_id") }
+
+  def apply(p: Pipeline, in: PCollection[Map[String,Any]])(implicit ctx: PipelineCtx): PCollection[Map[String,Any]] = {
+    // Build side-input: sample map keyed by S3 id field (if mapping provided)
+    val s3IdKey = idKey.map(_._2).getOrElse("id")
+    val s3Index: Map[Any, Map[String,Any]] = sample.flatMap { m =>
+      m.get(s3IdKey).map(_ -> m)
+    }.toMap
+
+    val s3Side = p.apply("CreateS3Index", org.apache.beam.sdk.transforms.Create.of(s3Index))
+      .apply("SingletonS3Index", View.asSingleton())
+
+    val mappingBC = g2sMapping
+
+    val out = in.apply("Reconcile per record", ParDo.of(new DoFn[Map[String,Any], Map[String,Any]] {
+      private var mismatches: Int = 0
+      private var checked: Int = 0
+      private var matched: Int = 0
+      private val maxSamples = 20
+      private val samples = scala.collection.mutable.ListBuffer.empty[String]
+
+      @ProcessElement
+      def proc(c: DoFn[Map[String,Any], Map[String,Any]]#ProcessContext): Unit = {
+        val rec = c.element()
+        checked += 1
+        val idx = c.sideInput(s3Side).asInstanceOf[java.util.Map[Any, Map[String,Any]]]
+        val scalaIdx: Map[Any, Map[String,Any]] = idx.asInstanceOf[java.util.Map[Any, Map[String,Any]]].asScala.toMap
+
+        val gIdKey = idKey.map(_._1).getOrElse("id")
+        val gid    = rec.getOrElse(gIdKey, null)
+
+        scalaIdx.get(gid) match {
+          case Some(s3rec) =>
+            val diffs = mappingBC.flatMap{ case (g,s) =>
+              val gv = rec.getOrElse(g, null)
+              val sv = s3rec.getOrElse(s, null)
+              if (String.valueOf(gv) != String.valueOf(sv))
+                Some(s"$g!=$s (gcp=$gv, s3=$sv)")
+              else None
+            }
+            if (diffs.nonEmpty) {
+              mismatches += 1
+              if (samples.size < maxSamples) {
+                samples += s\"{\"id\":\"$gid\",\"diffs\":\"${diffs.mkString(";")}\",\"gcp\":${rec.toString()},\"s3\":${s3rec.toString()}}\" 
+              }
+            } else {
+              matched += 1
+            }
+          case None =>
+            mismatches += 1
+            if (samples.size < maxSamples) {
+              samples += s\"{\"id\":\"$gid\",\"diffs\":\"missing_on_s3\"}\" 
+            }
+        }
+        c.output(rec)
       }
-    }))
+
+      @FinishBundle
+      def finish(c: DoFn[Map[String,Any], Map[String,Any]]#FinishBundleContext): Unit = {
+        val payload =
+          s\"{\"level\":\"INFO\",\"type\":\"reconcile\",\"zone\":\"$zone\",\"table\":\"$table\",\"checked\":$checked,\"matched\":$matched,\"mismatches\":$mismatches,\"mapping\":\"${mappingBC.mkString(",")}\",\"samples\":[${samples.mkString(",")}]}\" 
+        audit(payload)
+      }
+    }).withSideInputs(s3Side)))
+
+    out
   }
 }
--- a/framework/src/main/scala/com/analytics/framework/pipeline/stages/QualityStage.scala
+++ b/framework/src/main/scala/com/analytics/framework/pipeline/stages/QualityStage.scala
@@ -1,20 +1,37 @@
 package com.analytics.framework.pipeline.stages
-import com.analytics.framework.core.base.{BaseStage, PipelineCtx}
+
 import org.apache.beam.sdk.Pipeline
 import org.apache.beam.sdk.values.PCollection
 import org.apache.beam.sdk.transforms.{DoFn, ParDo}
-import org.apache.beam.sdk.transforms.DoFn.ProcessElement
-class QualityStage[T <: Map[String,Any]](notNullCols: List[String], audit: String => Unit)
-  extends BaseStage[T,T]{
-  override def name: String = "quality_stage"
-  def apply(p:Pipeline, in:PCollection[T])(implicit ctx:PipelineCtx): PCollection[T] = {
-    in.apply("dq-check", ParDo.of(new DoFn[T,T](){
-      @ProcessElement def proc(c: DoFn[T,T]#ProcessContext): Unit = {
+import com.analytics.framework.core.base.PipelineCtx
+import scala.collection.JavaConverters._
+
+class QualityStage[T <: Map[String,Any]](rules: QualityRules, audit: String => Unit) {
+
+  def apply(p: Pipeline, in: PCollection[Map[String,Any]])(implicit ctx: PipelineCtx): PCollection[Map[String,Any]] = {
+    val notNullCols = rules.notNull.getOrElse(Nil)
+    in.apply("DQ NotNull", ParDo.of(new DoFn[Map[String,Any], Map[String,Any]] {
+      @ProcessElement
+      def proc(c: DoFn[Map[String,Any], Map[String,Any]]#ProcessContext): Unit = {
         val m = c.element()
-        val fails = notNullCols.filter(col => !m.contains(col) || m.get(col)==null)
-        if (fails.nonEmpty) audit(s"""{"level":"WARN","type":"not_null","cols":"${fails.mkString(",")}","row":"$m"}""")
+        val fails = notNullCols.flatMap(col => if (!m.contains(col) || m(col) == null) Some(col) else None)
+        if (fails.nonEmpty) {
+          val line = s"""{"level":"WARN","type":"not_null","cols":"${fails.mkString(",")}","row":"$m"}"""
+          audit(line)
+        }
         c.output(m)
       }
     }))
   }
 }
+
+case class QualityRules(notNull: Option[List[String]])
+object RulesLoader {
+  def loadNotNullRules(path:String): QualityRules = {
+    val in = new java.io.FileInputStream(path)
+    val yaml = new org.yaml.snakeyaml.Yaml()
+    val data = yaml.load(in).asInstanceOf[java.util.Map[String,Any]]
+    val nn = Option(data.get("not_null")).map(_.asInstanceOf[java.util.List[String]].asScala.toList).getOrElse(Nil)
+    QualityRules(Some(nn))
+  }
+}
--- a/framework/src/main/scala/com/analytics/framework/utils/RawConfigLoader.scala
+++ b/framework/src/main/scala/com/analytics/framework/utils/RawConfigLoader.scala
@@ -1,5 +1,25 @@
 package com.analytics.framework.utils
-final case class RawIngestConfig()
-object RawConfigLoader{
-  def load(configPath:String): RawIngestConfig = RawIngestConfig()
+
+import scala.collection.JavaConverters._
+import org.yaml.snakeyaml.Yaml
+import java.io.FileInputStream
+
+case class RawIngestConfig(table:String,
+                           dataset:String,
+                           destination:String,
+                           mappings: Map[String,String],
+                           notNull: List[String])
+
+object RawConfigLoader {
+  def load(path:String): RawIngestConfig = {
+    val in = new FileInputStream(path)
+    val y  = new Yaml()
+    val data = y.load(in).asInstanceOf[java.util.Map[String,Any]]
+    val table       = String.valueOf(data.getOrDefault("table", ""))
+    val dataset     = String.valueOf(data.getOrDefault("dataset", ""))
+    val dest        = String.valueOf(data.getOrDefault("destination", "raw"))
+    val mappings    = Option(data.get("mappings")).map(_.asInstanceOf[java.util.Map[String,String]].asScala.toMap).getOrElse(Map.empty)
+    val notNull     = Option(data.get("not_null")).map(_.asInstanceOf[java.util.List[String]].asScala.toList).getOrElse(Nil)
+    RawIngestConfig(table, dataset, dest, mappings, notNull)
+  }
 }
--- a/README.md
+++ b/README.md
@@ -0,0 +1,9 @@
+# gpt5_helper_cdc_realtime
+
+This repo contains a minimal Apache Beam (Scala) framework and a sample **member** pipeline that ingests Pub/Sub notifications, fetches records, performs **quality checks**, **transforms**, and **reconciles** against S3 snapshots, then writes to BigQuery.
+
+## Projects
+- `framework/` – reusable stages, connectors, and utilities
+- `member-pipeline/` – example domain pipeline wiring
+
+See `docs/ARCHITECTURE.md` and `docs/DEVELOPMENT.md`.
--- a/docs/ARCHITECTURE.md
+++ b/docs/ARCHITECTURE.md
@@ -0,0 +1,15 @@
+# Architecture
+
+- **NotificationStage**: converts Pub/Sub messages to an internal record.
+- **MessageToRawStage**: parses incoming JSON message to a `Map[String,Any]`.
+- **QualityStage**: not-null checks based on YAML rules.
+- **TransformStage**: invokes domain `TransformModule` to produce structure/refined/analytics shapes.
+- **ReconcileStage**: compares GCP records vs S3 JSONL snapshot per table/zone and logs mismatches.
+- **BqWriteDynamicStage**: writes to BigQuery (templated table destination).
+
+## Key classes
+- `PipelineCtx`: ambient config (projectId, region, windowId, buckets...).
+- `TransformModule`: SPI for user-defined transforms.
+- `YamlLoader` / `RawConfigLoader`: YAML helpers to load config/rules.
+- `JsonDotPath`: extract with dot-path from JSON strings.
+- `S3JsonlReader`: read `.jsonl` files from S3 (or local mock) into Maps.
--- a/docs/DEVELOPMENT.md
+++ b/docs/DEVELOPMENT.md
@@ -0,0 +1,26 @@
+# Development
+
+## Prereqs
+- JDK 17
+- sbt 1.11.x
+- Google Cloud & AWS credentials if exercising BQ/S3 code
+
+## Build
+```bash
+sbt compile
+```
+
+## Run (membership example)
+Integration requires real cloud services; for local dev, use `mock_s3_dir` in config to point to local snapshot files:
+```
+aws_snapshot_s3: s3://bucket/path        # real
+# or
+mock_s3_dir: /absolute/path/to/snapshots  # local dev
+```
+
+## Testing
+Add unit tests under `framework/src/test/scala` and `member-pipeline/src/test/scala`.
+
+## Coding conventions
+- Scala 2.12, avoid `scala.jdk.CollectionConverters` (use `JavaConverters`).
+- All Beam stages should be **pure** in outputs and use `audit(String)` for side effects.
--- a/docs/OPERATIONS.md
+++ b/docs/OPERATIONS.md
@@ -0,0 +1,5 @@
+# Operations
+
+- Set `buckets.audit` in your config to receive audit logs.
+- Windows are identified by `windowId` (format configurable).
+- Reconciliation logs include counts and sample mismatches.
--- a/docs/INSTRUCTIONS.md
+++ b/docs/INSTRUCTIONS.md
@@ -0,0 +1,8 @@
+# Instructions
+
+This document explains how to extend the pipeline for new tables:
+
+1. Create YAML mapping for your table (mappings, not_null).
+2. Add a `TransformModule` implementing domain logic if needed.
+3. Wire it in `CommonRunner.scala` modules list.
+4. Provide S3 snapshots under `s3://.../{zone}/{table}/{yyyyMMddHHmm}/` or `mock_s3_dir`.
--- a/docs/CODE_VIZ.md
+++ b/docs/CODE_VIZ.md
@@ -0,0 +1,12 @@
+# Code Visualization
+
+```
+framework/
+  core/base/ (PipelineCtx, BaseStage)
+  connectors/ (S3JsonlReader, BigQueryDataFetcher)
+  pipeline/stages/ (Notification, MessageToRaw, FieldMap, Quality, Transform, Reconcile, BqRead/Write)
+  utils/ (YamlLoader, RawConfigLoader, JsonDotPath, SecretManagerUtil)
+member-pipeline/
+  transformations/ (MemberRefinedTransform, MemberAnalyticsTransform)
+  dataflow/ (MemberPipeline)
+```
