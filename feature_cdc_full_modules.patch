diff --git a/README.md b/README.md
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/README.md
@@ -0,0 +1,9 @@
+# GPT5 Helper CDC Realtime – Full Modules (Raw → Structure → Refined → Analytics)
+
+This branch adds a production-ready scaffold:
+- Region: `asia-southeast1` (Singapore)
+- Full zones with Reconcile + DQ + Audit per zone
+- Config-driven schema mapping + rules
+- Infra SQL for framework tables
+
+See `member-pipeline/config/pipeline_config.yaml` to set project/reconcile prefixes & rules.

diff --git a/build.sbt b/build.sbt
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/build.sbt
@@ -0,0 +1,36 @@
+ThisBuild / scalaVersion := "2.12.18"
+
+lazy val versions = new {
+  val beam = "2.58.0"
+  val gcloudBq = "2.43.2"
+  val slf4j = "2.0.12"
+  val gson = "2.10.1"
+  val snake = "2.0"
+  val awsSdk = "2.25.62"
+}
+
+lazy val root = (project in file("."))
+  .aggregate(framework, memberPipeline)
+  .settings(
+    name := "gpt5-helper-cdc-realtime",
+    version := "0.3.0"
+  )
+
+lazy val commonLibs = Seq(
+  "org.apache.beam" % "beam-sdks-java-core" % versions.beam,
+  "org.apache.beam" % "beam-runners-google-cloud-dataflow-java" % versions.beam,
+  "com.google.cloud" % "google-cloud-bigquery" % versions.gcloudBq,
+  "software.amazon.awssdk" % "s3" % versions.awsSdk,
+  "com.google.code.gson" % "gson" % versions.gson,
+  "org.yaml" % "snakeyaml" % versions.snake,
+  "org.slf4j" % "slf4j-api" % versions.slf4j
+)
+
+lazy val framework = (project in file("framework")).settings(
+  name := "cdc-framework",
+  libraryDependencies ++= commonLibs
+)
+
+lazy val memberPipeline = (project in file("member-pipeline")).dependsOn(framework).settings(
+  name := "member-pipeline"
+)

diff --git a/infrastructure/sql/framework_tables.sql b/infrastructure/sql/framework_tables.sql
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/infrastructure/sql/framework_tables.sql
@@ -0,0 +1,44 @@
+-- Framework tables for audit & dq & reconcile (create per domain e.g. member_framework)
+CREATE OR REPLACE TABLE `${PROJECT_ID}.${DOMAIN}_framework.pipeline_log` (
+  log_id STRING NOT NULL,
+  pipeline_name STRING NOT NULL,
+  domain STRING NOT NULL,
+  zone STRING NOT NULL,
+  table_name STRING NOT NULL,
+  window_start TIMESTAMP,
+  window_end TIMESTAMP,
+  status STRING NOT NULL,
+  metrics JSON,
+  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
+)
+PARTITION BY DATE(created_at)
+CLUSTER BY domain, zone, table_name;
+
+CREATE OR REPLACE TABLE `${PROJECT_ID}.${DOMAIN}_framework.reconcile_log` (
+  reconcile_id STRING NOT NULL,
+  domain STRING NOT NULL,
+  zone STRING NOT NULL,
+  table_name STRING NOT NULL,
+  window_start TIMESTAMP,
+  window_end TIMESTAMP,
+  gcp_count INT64,
+  aws_count INT64,
+  count_match BOOL,
+  mismatch_samples ARRAY<JSON>,
+  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
+)
+PARTITION BY DATE(created_at)
+CLUSTER BY domain, zone, table_name;
+
+CREATE OR REPLACE TABLE `${PROJECT_ID}.${DOMAIN}_framework.data_quality_log` (
+  quality_id STRING NOT NULL,
+  domain STRING NOT NULL,
+  zone STRING NOT NULL,
+  table_name STRING NOT NULL,
+  rule_name STRING NOT NULL,
+  passed BOOL,
+  details JSON,
+  created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
+)
+PARTITION BY DATE(created_at)
+CLUSTER BY domain, zone, table_name;

diff --git a/framework/src/main/scala/com/analytics/framework/utils/ConfigLoader.scala b/framework/src/main/scala/com/analytics/framework/utils/ConfigLoader.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/utils/ConfigLoader.scala
@@ -0,0 +1,14 @@
+package com.analytics.framework.utils
+
+import org.yaml.snakeyaml.Yaml
+import java.io.FileInputStream
+import scala.jdk.CollectionConverters._
+
+object ConfigLoader {
+  def load(path: String): java.util.Map[String, Object] = {
+    val yaml = new Yaml()
+    val in = new FileInputStream(path)
+    try yaml.load(in).asInstanceOf[java.util.Map[String, Object]]
+    finally in.close()
+  }
+}

diff --git a/framework/src/main/scala/com/analytics/framework/utils/HashUtils.scala b/framework/src/main/scala/com/analytics/framework/utils/HashUtils.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/utils/HashUtils.scala
@@ -0,0 +1,11 @@
+package com.analytics.framework.utils
+
+import java.security.MessageDigest
+
+object HashUtils {
+  def md5(s: String): String = {
+    val md = MessageDigest.getInstance("MD5")
+    md.update(s.getBytes("UTF-8"))
+    md.digest.map("%02x".format(_)).mkString
+  }
+}

diff --git a/framework/src/main/scala/com/analytics/framework/connectors/BigQueryDataFetcher.scala b/framework/src/main/scala/com/analytics/framework/connectors/BigQueryDataFetcher.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/connectors/BigQueryDataFetcher.scala
@@ -0,0 +1,42 @@
+package com.analytics.framework.connectors
+
+import org.apache.beam.sdk.transforms.DoFn
+import com.google.cloud.bigquery._
+import com.analytics.framework.pipeline.core.NotificationEvent
+import scala.jdk.CollectionConverters._
+
+case class FetchedData(recordId: String, row: java.util.Map[String, AnyRef])
+
+class BigQueryDataFetcher(projectId: String, dataset: String, table: String, idColumn: String)
+  extends DoFn[NotificationEvent, FetchedData] {
+
+  @transient private var bq: BigQuery = _
+
+  @org.apache.beam.sdk.transforms.DoFn.Setup
+  def setup(): Unit = {
+    bq = BigQueryOptions.newBuilder().setProjectId(projectId).build().getService
+  }
+
+  @org.apache.beam.sdk.transforms.DoFn.ProcessElement
+  def process(ctx: DoFn[NotificationEvent, FetchedData]#ProcessContext): Unit = {
+    val e = ctx.element()
+    val ids = e.recordIds.asScala.map(id => s"'$id'").mkString(",")
+    val sql = s"""SELECT * FROM `%s.%s.%s` WHERE %s IN (%s)""".format(projectId, dataset, table, idColumn, ids)
+
+    val config = QueryJobConfiguration.newBuilder(sql).setUseLegacySql(false).build()
+    val result = bq.query(config)
+
+    val schema = result.getSchema
+    for (row <- result.iterateAll().asScala) {
+      val map = new java.util.HashMap[String, AnyRef]()
+      var idx = 0
+      for (field <- schema.getFields.asScala) {
+        val v = row.get(idx)
+        map.put(field.getName, if (v == null || v.isNull) null else v.getValue)
+        idx += 1
+      }
+      val ridField = Option(map.get(idColumn)).map(_.toString).getOrElse(java.util.UUID.randomUUID().toString)
+      ctx.output(FetchedData(ridField, map))
+    }
+  }
+}

diff --git a/framework/src/main/scala/com/analytics/framework/modules/reconciliation/S3SnapshotReader.scala b/framework/src/main/scala/com/analytics/framework/modules/reconciliation/S3SnapshotReader.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/modules/reconciliation/S3SnapshotReader.scala
@@ -0,0 +1,23 @@
+package com.analytics.framework.modules.reconciliation
+
+import software.amazon.awssdk.services.s3.S3Client
+import software.amazon.awssdk.services.s3.model._
+import scala.jdk.CollectionConverters._
+import java.nio.charset.StandardCharsets
+
+class S3SnapshotReader(bucket: String, prefix: String) {
+  @transient private var s3: S3Client = _
+
+  def init(): Unit = { s3 = S3Client.create() }
+
+  def listKeys(limit: Int = 64): List[String] = {
+    val req = ListObjectsV2Request.builder().bucket(bucket).prefix(prefix).maxKeys(limit).build()
+    s3.listObjectsV2(req).contents().asScala.map(_.key()).toList
+  }
+
+  def readObjectLines(key: String): Iterator[String] = {
+    val get = GetObjectRequest.builder().bucket(bucket).key(key).build()
+    val in = s3.getObject(get)
+    scala.io.Source.fromInputStream(in, StandardCharsets.UTF_8.name()).getLines()
+  }
+}

diff --git a/framework/src/main/scala/com/analytics/framework/modules/reconciliation/SchemaMapper.scala b/framework/src/main/scala/com/analytics/framework/modules/reconciliation/SchemaMapper.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/modules/reconciliation/SchemaMapper.scala
@@ -0,0 +1,8 @@
+package com.analytics.framework.modules.reconciliation
+
+final class SchemaMapper(tableMappings: Map[String, Map[String, String]]) {
+  def mapAwsToGcp(table: String, row: Map[String, Any]): Map[String, Any] = {
+    val m = tableMappings.getOrElse(table, Map.empty)
+    row.map { case (awsCol, v) => m.getOrElse(awsCol, awsCol) -> v }
+  }
+}

diff --git a/framework/src/main/scala/com/analytics/framework/modules/reconciliation/ReconciliationEngine.scala b/framework/src/main/scala/com/analytics/framework/modules/reconciliation/ReconciliationEngine.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/modules/reconciliation/ReconciliationEngine.scala
@@ -0,0 +1,81 @@
+package com.analytics.framework.modules.reconciliation
+
+import org.apache.beam.sdk.transforms.DoFn
+import org.apache.beam.sdk.transforms.DoFn.{ProcessElement, Setup}
+import org.apache.beam.sdk.transforms.windowing.{BoundedWindow, IntervalWindow}
+import com.analytics.framework.pipeline.core.CDCRecord
+import scala.jdk.CollectionConverters._
+import java.time.{Instant => JInstant}
+
+final case class ReconcileResult(
+  recordId: String,
+  tableName: String,
+  zone: String,
+  gcpCount: Long,
+  awsCount: Long,
+  countMatch: Boolean,
+  mismatchSamples: List[Map[String, Any]],
+  windowStart: String,
+  windowEnd: String,
+  ts: Long
+)
+
+class ReconciliationEngine(
+  bucket: String,
+  prefix: String,
+  idColumn: String,
+  tableMappings: Map[String, Map[String, String]],
+  sampleLimit: Int = 5
+) extends DoFn[CDCRecord, CDCRecord] {
+
+  @transient private var s3: S3SnapshotReader = _
+  @transient private var mapper: SchemaMapper = _
+
+  @Setup def setup(): Unit = {
+    s3 = new S3SnapshotReader(bucket, prefix)
+    s3.init()
+    mapper = new SchemaMapper(tableMappings)
+  }
+
+  @ProcessElement
+  def process(ctx: DoFn[CDCRecord, CDCRecord]#ProcessContext, window: BoundedWindow): Unit = {
+    val w = window.asInstanceOf[IntervalWindow]
+    val rec = ctx.element()
+    val rid = rec.recordId
+    val table = rec.tableName
+
+    val keys = s3.listKeys(limit = 32).filter(_.nonEmpty)
+    val awsRows: List[Map[String, Any]] =
+      keys.iterator.flatMap { k =>
+        s3.readObjectLines(k).take(2000).flatMap { line =>
+          // Expect JSONL; adapt as needed
+          import io.circe.parser._
+          parse(line).toOption.flatMap(_.asObject).map(_.toMap.map {
+            case (k,v) => k -> (if(v.isNull) null else v.toString)
+          }).filter(_.getOrElse(idColumn, "") == rid)
+        }
+      }.take(sampleLimit).toList
+
+    val mappedAwsRows = awsRows.map(r => mapper.mapAwsToGcp(table, r))
+    val gcpData = rec.data.asScala.toMap
+
+    val gcpCount = 1L
+    val awsCount = mappedAwsRows.size.toLong
+    val countMatch = gcpCount == awsCount
+
+    val diffs =
+      mappedAwsRows.take(sampleLimit).flatMap { awsRow =>
+        val cols = (gcpData.keySet ++ awsRow.keySet).filterNot(_.startsWith("_"))
+        val bad = cols.filter { c =>
+          val gv = Option(gcpData.getOrElse(c, null)).map(_.toString).orNull
+          val av = Option(awsRow.getOrElse(c, null)).map(_.toString).orNull
+          gv != av
+        }
+        if (bad.nonEmpty) Some(Map("record_id" -> rid, "columns" -> bad.mkString(","))) else None
+      }
+
+    // (Module 04) send ReconcileResult to BQ
+    // println(s"Recon: table=$table zone=${rec.zone} rid=$rid gcp=$gcpCount aws=$awsCount match=$countMatch diffs=$diffs")
+    ctx.output(rec)
+  }
+}

diff --git a/framework/src/main/scala/com/analytics/framework/modules/reconciliation/ReconciliationAuditor.scala b/framework/src/main/scala/com/analytics/framework/modules/reconciliation/ReconciliationAuditor.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/modules/reconciliation/ReconciliationAuditor.scala
@@ -0,0 +1,33 @@
+package com.analytics.framework.modules.reconciliation
+
+import org.apache.beam.sdk.transforms.DoFn
+import com.google.cloud.bigquery._
+import scala.jdk.CollectionConverters._
+
+class ReconciliationAuditor(projectId: String, frameworkDataset: String)
+  extends DoFn[ReconcileResult, Void] {
+
+  @transient private var bq: BigQuery = _
+
+  @org.apache.beam.sdk.transforms.DoFn.Setup
+  def setup(): Unit = { bq = BigQueryOptions.newBuilder().setProjectId(projectId).build().getService }
+
+  @org.apache.beam.sdk.transforms.DoFn.ProcessElement
+  def process(ctx: DoFn[ReconcileResult, Void]#ProcessContext): Unit = {
+    val r = ctx.element()
+    val tableId = TableId.of(frameworkDataset, "reconcile_log")
+    val row = Map(
+      "reconcile_id" -> java.util.UUID.randomUUID().toString,
+      "domain" -> frameworkDataset.replaceAll("_framework$", ""),
+      "zone" -> r.zone,
+      "table_name" -> r.tableName,
+      "window_start" -> r.windowStart,
+      "window_end" -> r.windowEnd,
+      "gcp_count" -> Long.box(r.gcpCount),
+      "aws_count" -> Long.box(r.awsCount),
+      "count_match" -> Boolean.box(r.countMatch),
+      "mismatch_samples" -> r.mismatchSamples.map(_.toString).asJava
+    ).asJava.asInstanceOf[java.util.Map[String, AnyRef]]
+    bq.insertAll(InsertAllRequest.newBuilder(tableId).addRow(row).build())
+  }
+}

diff --git a/framework/src/main/scala/com/analytics/framework/modules/quality/DataQualityEngine.scala b/framework/src/main/scala/com/analytics/framework/modules/quality/DataQualityEngine.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/modules/quality/DataQualityEngine.scala
@@ -0,0 +1,37 @@
+package com.analytics.framework.modules.quality
+
+import org.apache.beam.sdk.transforms.DoFn
+import com.analytics.framework.pipeline.core.CDCRecord
+import scala.util.matching.Regex
+import scala.jdk.CollectionConverters._
+
+case class DQRule(name: String, columns: List[String], pattern: Option[String] = None, min: Option[Double] = None, max: Option[Double] = None, unique: Boolean = false)
+
+class DataQualityEngine(rules: List[DQRule]) extends DoFn[CDCRecord, CDCRecord] {
+  @org.apache.beam.sdk.transforms.DoFn.ProcessElement
+  def process(ctx: DoFn[CDCRecord, CDCRecord]#ProcessContext): Unit = {
+    val rec = ctx.element()
+    var passed = true
+
+    rules.foreach { r =>
+      r.name match {
+        case "not_null" =>
+          passed &&= r.columns.forall(c => rec.data.containsKey(c) && rec.data.get(c) != null)
+        case "regex" =>
+          val re = r.pattern.map(_.r).getOrElse(".*".r)
+          passed &&= r.columns.forall { c =>
+            val v = Option(rec.data.get(c)).map(_.toString).getOrElse("")
+            re.pattern.matcher(v).matches()
+          }
+        case "range" =>
+          passed &&= r.columns.forall { c =>
+            val v = Option(rec.data.get(c)).map(_.toString.toDouble).getOrElse(Double.NaN)
+            !v.isNaN && r.min.forall(v >= _) && r.max.forall(v <= _)
+          }
+        case _ => ()
+      }
+    }
+
+    if (passed) ctx.output(rec) // TODO: side output failed rows
+  }
+}

diff --git a/framework/src/main/scala/com/analytics/framework/modules/audit/AuditLogger.scala b/framework/src/main/scala/com/analytics/framework/modules/audit/AuditLogger.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/modules/audit/AuditLogger.scala
@@ -0,0 +1,29 @@
+package com.analytics.framework.modules.audit
+
+import org.apache.beam.sdk.transforms.DoFn
+import com.analytics.framework.pipeline.core.CDCRecord
+import com.google.cloud.bigquery._
+import scala.jdk.CollectionConverters._
+
+class AuditLogger(projectId: String, frameworkDataset: String, tableName: String, zone: String)
+  extends DoFn[CDCRecord, CDCRecord] {
+
+  @transient private var bq: BigQuery = _
+  @org.apache.beam.sdk.transforms.DoFn.Setup
+  def setup(): Unit = { bq = BigQueryOptions.newBuilder().setProjectId(projectId).build().getService }
+
+  @org.apache.beam.sdk.transforms.DoFn.ProcessElement
+  def process(ctx: DoFn[CDCRecord, CDCRecord]#ProcessContext): Unit = {
+    val tableId = TableId.of(frameworkDataset, "pipeline_log")
+    val row = Map(
+      "log_id" -> java.util.UUID.randomUUID().toString,
+      "pipeline_name" -> s"${tableName}_${zone}",
+      "domain" -> frameworkDataset.replaceAll("_framework$", ""),
+      "zone" -> zone,
+      "table_name" -> tableName,
+      "status" -> "SUCCESS"
+    ).asJava.asInstanceOf[java.util.Map[String, AnyRef]]
+    bq.insertAll(InsertAllRequest.newBuilder(tableId).addRow(row).build())
+    ctx.output(ctx.element())
+  }
+}

diff --git a/framework/src/main/scala/com/analytics/framework/pipeline/core/NotificationProcessor.scala b/framework/src/main/scala/com/analytics/framework/pipeline/core/NotificationProcessor.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/pipeline/core/NotificationProcessor.scala
@@ -0,0 +1,52 @@
+package com.analytics.framework.pipeline.core
+
+import org.apache.beam.sdk.transforms.DoFn
+import org.apache.beam.sdk.io.gcp.pubsub.PubsubMessage
+import com.google.gson.{Gson, JsonParser, JsonElement}
+import scala.jdk.CollectionConverters._
+
+class NotificationProcessor(table: String) extends DoFn[PubsubMessage, NotificationEvent] {
+  @transient private var gson: Gson = _
+  @transient private var parser: JsonParser = _
+
+  @org.apache.beam.sdk.transforms.DoFn.Setup
+  def setup(): Unit = {
+    gson = new Gson()
+    parser = new JsonParser()
+  }
+
+  @org.apache.beam.sdk.transforms.DoFn.ProcessElement
+  def process(ctx: DoFn[PubsubMessage, NotificationEvent]#ProcessContext): Unit = {
+    val msg = ctx.element()
+    val payload = new String(msg.getPayload, java.nio.charset.StandardCharsets.UTF_8)
+    val attr = msg.getAttributeMap.asScala
+
+    if (attr.get("table_name").exists(_ == table)) {
+      val json: JsonElement = parser.parse(payload)
+      val recordIds: java.util.List[String] =
+        if (json.isJsonArray) json.getAsJsonArray.iterator().asScala.map(_.getAsString).toList.asJava
+        else if (json.isJsonObject && json.getAsJsonObject.has("record_ids"))
+          json.getAsJsonObject.getAsJsonArray("record_ids").iterator().asScala.map(_.getAsString).toList.asJava
+        else java.util.Arrays.asList(json.getAsJsonObject.get("record_id").getAsString)
+
+      val out = new NotificationEvent(
+        attr.getOrElse("message_id", java.util.UUID.randomUUID().toString),
+        attr.getOrElse("event_type", "update"),
+        table,
+        recordIds,
+        attr.getOrElse("event_timestamp", java.time.Instant.now().toString),
+        payload
+      )
+      ctx.output(out)
+    }
+  }
+}
+
+case class NotificationEvent(
+  messageId: String,
+  eventType: String,
+  tableName: String,
+  recordIds: java.util.List[String],
+  timestamp: String,
+  content: String
+)

diff --git a/framework/src/main/scala/com/analytics/framework/pipeline/core/CDCDetector.scala b/framework/src/main/scala/com/analytics/framework/pipeline/core/CDCDetector.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/pipeline/core/CDCDetector.scala
@@ -0,0 +1,19 @@
+package com.analytics.framework.pipeline.core
+
+import org.apache.beam.sdk.transforms.DoFn
+import com.analytics.framework.connectors.FetchedData
+
+case class CDCRecord(recordId: String, op: String, data: java.util.Map[String, AnyRef], tableName: String, zone: String)
+
+class CDCDetector(opField: String = "op_type", deleteFlag: String = "is_deleted", tableNameField: String = "_table_name", zoneField: String = "_zone")
+  extends DoFn[FetchedData, CDCRecord] {
+  @org.apache.beam.sdk.transforms.DoFn.ProcessElement
+  def process(ctx: DoFn[FetchedData, CDCRecord]#ProcessContext): Unit = {
+    val f = ctx.element()
+    val opRaw = Option(f.row.get(opField)).map(_.toString)
+    val del = Option(f.row.get(deleteFlag)).exists(v => v != null && v.toString.equalsIgnoreCase("true"))
+    val op = if (del) "delete" else opRaw.getOrElse("upsert")
+    val tableName = Option(f.row.get(tableNameField)).map(_.toString).getOrElse("unknown")
+    ctx.output(CDCRecord(f.recordId, op, f.row, tableName, "raw"))
+  }
+}

diff --git a/framework/src/main/scala/com/analytics/framework/pipeline/core/RawInserter.scala b/framework/src/main/scala/com/analytics/framework/pipeline/core/RawInserter.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/pipeline/core/RawInserter.scala
@@ -0,0 +1,29 @@
+package com.analytics.framework.pipeline.core
+
+import org.apache.beam.sdk.transforms.DoFn
+import com.google.cloud.bigquery._
+import scala.jdk.CollectionConverters._
+
+class RawInserter(projectId: String, dataset: String, table: String)
+  extends DoFn[CDCRecord, CDCRecord] {
+
+  @transient private var bq: BigQuery = _
+  @org.apache.beam.sdk.transforms.DoFn.Setup
+  def setup(): Unit = {
+    bq = BigQueryOptions.newBuilder().setProjectId(projectId).build().getService
+  }
+
+  @org.apache.beam.sdk.transforms.DoFn.ProcessElement
+  def process(ctx: DoFn[CDCRecord, CDCRecord]#ProcessContext): Unit = {
+    val rec = ctx.element()
+    val tableId = TableId.of(dataset, s"${table}_raw")
+    val rowContent = new java.util.HashMap[String, AnyRef](rec.data)
+    rowContent.put("_event_op", rec.op)
+    rowContent.put("_ingest_ts", java.time.Instant.now().toString)
+    rowContent.put("_table_name", rec.tableName)
+    rowContent.put("_zone", "raw")
+    val insertAllRequest = InsertAllRequest.newBuilder(tableId).addRow(rowContent.asInstanceOf[java.util.Map[String, AnyRef]]).build()
+    val response = bq.insertAll(insertAllRequest)
+    if (!response.hasErrors) ctx.output(rec.copy(zone = "raw"))
+  }
+}

diff --git a/framework/src/main/scala/com/analytics/framework/pipeline/core/PipelineOrchestrator.scala b/framework/src/main/scala/com/analytics/framework/pipeline/core/PipelineOrchestrator.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/pipeline/core/PipelineOrchestrator.scala
@@ -0,0 +1,119 @@
+package com.analytics.framework.pipeline.core
+
+import org.apache.beam.sdk.Pipeline
+import org.apache.beam.sdk.options.PipelineOptionsFactory
+import org.apache.beam.sdk.transforms.{ParDo}
+import org.apache.beam.sdk.io.gcp.pubsub.{PubsubIO, PubsubMessage}
+import org.apache.beam.sdk.transforms.windowing._
+import org.joda.time.Duration
+
+import com.analytics.framework.utils.ConfigLoader
+import com.analytics.framework.connectors.BigQueryDataFetcher
+import com.analytics.framework.modules.reconciliation.{ReconciliationEngine, SchemaMapper}
+import com.analytics.framework.modules.quality.{DataQualityEngine, DQRule}
+import com.analytics.framework.modules.audit.AuditLogger
+import com.analytics.framework.transform.{StructureTransformer, RefinedTransformation, AnalyticsTransformation}
+import scala.jdk.CollectionConverters._
+
+class PipelineOrchestrator(
+  projectId: String,
+  domain: String,
+  table: String,
+  configPath: String
+) {
+
+  private val cfg = ConfigLoader.load(configPath)
+  private val jobName = s"${domain}-${table}-cdc-full"
+
+  def build(): Pipeline = {
+    val opts = PipelineOptionsFactory.create()
+    val df = opts.as(classOf[org.apache.beam.runners.dataflow.options.DataflowPipelineOptions])
+    df.setProject(projectId)
+    df.setRegion("asia-southeast1")
+    df.setJobName(jobName)
+    df.setStreaming(true)
+    df.setEnableStreamingEngine(true)
+    val p = Pipeline.create(opts)
+
+    val rawDataset = cfg.getOrDefault("raw_dataset", s"${domain}_raw").toString
+    val structureDataset = cfg.getOrDefault("structure_dataset", s"${domain}_structure").toString
+    val refinedDataset = cfg.getOrDefault("refined_dataset", s"${domain}_refined").toString
+    val analyticsDataset = cfg.getOrDefault("analytics_dataset", s"${domain}_analytics").toString
+    val frameworkDataset = cfg.getOrDefault("framework_dataset", s"${domain}_framework").toString
+    val idColumn = cfg.getOrDefault("id_column", "record_id").toString
+
+    val notifications = p
+      .apply("ReadNotifications",
+        PubsubIO.readMessagesWithAttributes()
+          .fromSubscription(s"projects/$projectId/subscriptions/${domain}-notification-sub")
+          .withIdAttribute("message_id")
+          .withTimestampAttribute("event_timestamp"))
+      .apply("Window1s",
+        Window.into[PubsubMessage](FixedWindows.of(Duration.standardSeconds(1)))
+          .triggering(
+            AfterWatermark.pastEndOfWindow()
+              .withEarlyFirings(AfterProcessingTime.pastFirstElementInPane().plusDelayOf(Duration.standardMillis(500)))
+              .withLateFirings(AfterPane.elementCountAtLeast(1))
+          ).withAllowedLateness(Duration.standardMinutes(5))
+          .discardingFiredPanes()
+      )
+      .apply("ProcessNotifications", ParDo.of(new NotificationProcessor(table)))
+
+    val fetched = notifications.apply("FetchFromBQ",
+      ParDo.of(new BigQueryDataFetcher(projectId, rawDataset, table, idColumn)))
+
+    val cdc = fetched.apply("CDCDetect", ParDo.of(new CDCDetector()))
+
+    // RAW
+    val raw = cdc
+      .apply("InsertRaw", ParDo.of(new RawInserter(projectId, rawDataset, table)))
+      .apply("AuditRaw", ParDo.of(new AuditLogger(projectId, frameworkDataset, table, "raw")))
+
+    val reconConf = cfg.get("reconciler").asInstanceOf[java.util.Map[String, Object]]
+    val awsBucket = reconConf.get("aws_bucket").toString
+    def prefix(z:String) = reconConf.getOrDefault(s"aws_prefix_${z}", s"path/to/${domain}/${z}/").toString
+
+    val mapping = Option(reconConf.get("mapping_schemas"))
+      .map(_.asInstanceOf[java.util.Map[String, java.util.Map[String, String]]]
+      .asScala.view.mapValues(_.asScala.toMap).toMap).getOrElse(Map.empty[String, Map[String, String]])
+
+    val rawRecon = raw.apply("ReconcileRaw",
+      ParDo.of(new ReconciliationEngine(awsBucket, prefix("raw"), idColumn, mapping)))
+
+    // DQ
+    val dqConf = cfg.get("dq").asInstanceOf[java.util.Map[String, Object]]
+    val notNullCols = Option(dqConf.get("not_null")).map(_.asInstanceOf[java.util.List[String]].asScala.toList).getOrElse(Nil)
+    val rules = (if (notNullCols.nonEmpty) List(DQRule("not_null", notNullCols)) else Nil)
+    val rawDQ = rawRecon.apply("DQRaw", ParDo.of(new DataQualityEngine(rules)))
+      .apply("AuditRawDQ", ParDo.of(new AuditLogger(projectId, frameworkDataset, table, "raw_dq")))
+
+    // STRUCTURE
+    val structured = rawDQ.apply("StructureTransform", ParDo.of(new StructureTransformer(Map.empty)))
+      .apply("AuditStructure", ParDo.of(new AuditLogger(projectId, frameworkDataset, table, "structure")))
+
+    val structureRecon = structured.apply("ReconcileStructure",
+      ParDo.of(new ReconciliationEngine(awsBucket, prefix("structure"), idColumn, mapping)))
+    val structureDQ = structureRecon.apply("DQStructure", ParDo.of(new DataQualityEngine(rules)))
+      .apply("AuditStructureDQ", ParDo.of(new AuditLogger(projectId, frameworkDataset, table, "structure_dq")))
+
+    // REFINED
+    val refined = structureDQ.apply("RefinedTransform", ParDo.of(new RefinedTransformation()))
+      .apply("AuditRefined", ParDo.of(new AuditLogger(projectId, frameworkDataset, table, "refined")))
+
+    val refinedRecon = refined.apply("ReconcileRefined",
+      ParDo.of(new ReconciliationEngine(awsBucket, prefix("refined"), idColumn, mapping)))
+    val refinedDQ = refinedRecon.apply("DQRefined", ParDo.of(new DataQualityEngine(rules)))
+      .apply("AuditRefinedDQ", ParDo.of(new AuditLogger(projectId, frameworkDataset, table, "refined_dq")))
+
+    // ANALYTICS
+    val analytics = refinedDQ.apply("AnalyticsTransform", ParDo.of(new AnalyticsTransformation()))
+      .apply("AuditAnalytics", ParDo.of(new AuditLogger(projectId, frameworkDataset, table, "analytics")))
+
+    val analyticsRecon = analytics.apply("ReconcileAnalytics",
+      ParDo.of(new ReconciliationEngine(awsBucket, prefix("analytics"), idColumn, mapping)))
+    val analyticsDQ = analyticsRecon.apply("DQAnalytics", ParDo.of(new DataQualityEngine(rules)))
+      .apply("AuditAnalyticsDQ", ParDo.of(new AuditLogger(projectId, frameworkDataset, table, "analytics_dq")))
+
+    p
+  }
+}

diff --git a/framework/src/main/scala/com/analytics/framework/transform/StructureTransformer.scala b/framework/src/main/scala/com/analytics/framework/transform/StructureTransformer.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/transform/StructureTransformer.scala
@@ -0,0 +1,26 @@
+package com.analytics.framework.transform
+
+import org.apache.beam.sdk.transforms.DoFn
+import com.analytics.framework.pipeline.core.CDCRecord
+import scala.jdk.CollectionConverters._
+
+class StructureTransformer(mapping: Map[String, String]) extends DoFn[CDCRecord, CDCRecord] {
+  @org.apache.beam.sdk.transforms.DoFn.ProcessElement
+  def process(ctx: DoFn[CDCRecord, CDCRecord]#ProcessContext): Unit = {
+    val rec = ctx.element()
+    val data = new java.util.HashMap[String, AnyRef](rec.data)
+    mapping.foreach { case (col, typ) =>
+      if (data.containsKey(col) && data.get(col) != null) {
+        val v = data.get(col).toString
+        typ.toLowerCase match {
+          case "string" => data.put(col, v)
+          case "int64"  => data.put(col, Long.box(v.toLong))
+          case "float64"=> data.put(col, Double.box(v.toDouble))
+          case "bool"   => data.put(col, Boolean.box(v.equalsIgnoreCase("true")))
+          case _        => data.put(col, v)
+        }
+      }
+    }
+    ctx.output(rec.copy(data = data, zone = "structure"))
+  }
+}

diff --git a/framework/src/main/scala/com/analytics/framework/transform/RefinedTransformation.scala b/framework/src/main/scala/com/analytics/framework/transform/RefinedTransformation.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/transform/RefinedTransformation.scala
@@ -0,0 +1,11 @@
+package com.analytics.framework.transform
+
+import org.apache.beam.sdk.transforms.DoFn
+import com.analytics.framework.pipeline.core.CDCRecord
+
+class RefinedTransformation extends DoFn[CDCRecord, CDCRecord] {
+  @org.apache.beam.sdk.transforms.DoFn.ProcessElement
+  def process(ctx: DoFn[CDCRecord, CDCRecord]#ProcessContext): Unit = {
+    ctx.output(ctx.element().copy(zone = "refined"))
+  }
+}

diff --git a/framework/src/main/scala/com/analytics/framework/transform/AnalyticsTransformation.scala b/framework/src/main/scala/com/analytics/framework/transform/AnalyticsTransformation.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/framework/src/main/scala/com/analytics/framework/transform/AnalyticsTransformation.scala
@@ -0,0 +1,11 @@
+package com.analytics.framework.transform
+
+import org.apache.beam.sdk.transforms.DoFn
+import com.analytics.framework.pipeline.core.CDCRecord
+
+class AnalyticsTransformation extends DoFn[CDCRecord, CDCRecord] {
+  @org.apache.beam.sdk.transforms.DoFn.ProcessElement
+  def process(ctx: DoFn[CDCRecord, CDCRecord]#ProcessContext): Unit = {
+    ctx.output(ctx.element().copy(zone = "analytics"))
+  }
+}

diff --git a/member-pipeline/src/main/scala/com/analytics/member/dataflow/MemberPipeline.scala b/member-pipeline/src/main/scala/com/analytics/member/dataflow/MemberPipeline.scala
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/member-pipeline/src/main/scala/com/analytics/member/dataflow/MemberPipeline.scala
@@ -0,0 +1,16 @@
+package com.analytics.member.dataflow
+
+import com.analytics.framework.pipeline.core.PipelineOrchestrator
+
+object MemberPipeline {
+  def main(args: Array[String]): Unit = {
+    val projectId = sys.env.getOrElse("GCP_PROJECT", "your-project-id")
+    val domain = sys.props.getOrElse("domain", "member")
+    val table = sys.props.getOrElse("table", "users")
+    val configPath = sys.props.getOrElse("config", "member-pipeline/config/pipeline_config.yaml")
+
+    val orch = new PipelineOrchestrator(projectId, domain, table, configPath)
+    val p = orch.build()
+    p.run()
+  }
+}

diff --git a/member-pipeline/config/pipeline_config.yaml b/member-pipeline/config/pipeline_config.yaml
new file mode 100644
index 0000000..e69de29
--- /dev/null
+++ b/member-pipeline/config/pipeline_config.yaml
@@ -0,0 +1,25 @@
+project_id: your-project-id
+region: asia-southeast1
+domain: member
+table: users
+id_column: record_id
+raw_dataset: member_raw
+structure_dataset: member_structure
+refined_dataset: member_refined
+analytics_dataset: member_analytics
+framework_dataset: member_framework
+
+reconciler:
+  aws_bucket: your-aws-bucket
+  aws_prefix_raw: "eagleeye/member/raw/"
+  aws_prefix_structure: "eagleeye/member/structure/"
+  aws_prefix_refined: "eagleeye/member/refined/"
+  aws_prefix_analytics: "eagleeye/member/analytics/"
+  mapping_schemas:
+    users:
+      user_id:  record_id
+      user_name: name
+      user_email: email
+
+dq:
+  not_null: ["record_id", "email"]

